{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064aa5c7",
   "metadata": {},
   "source": [
    "### 1. Convert all xlsx to csv\n",
    "Background:\n",
    "- Downloaded all datasets related to Passaic River sampling from https://sharepoint.ourpassaic.org/SitePages/Passaic%20River%20Datasets to \"xlsx_to_convert\" directory. Included: sediment, water column, biota sampling. We will need to review the datasets to determine if they arre alike enough to analyze using the same comparisons.\n",
    "- Excluded all datasets related to Newark Bay (OU3) because Newark Bay is outside our geographic scope.\n",
    "- Excluded all Bathymetry (mapping the bottom of the river) datasets because we aren't running an analysis on the physical features or water flow of the River.\n",
    "- Added all Microsoft Access datasets to a separate directory (\"access_to_convert\") because those will need a separate conversion process.\n",
    "\n",
    "\n",
    "Now attempting to add a function to open the xlsx datasets from \"xlsx_to_convert\" directory and save as csv to the \"rawdata\" directory\n",
    "-LK 4/2/2022 at 8am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1317b2ea-033b-4c27-92af-ab3cb2c1d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0fca6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_xlsx_to_csv(filename):\n",
    "    filepath = Path(f'../rawdata/xlsx_to_convert/{filename}')\n",
    "    csv_data = pd.read_excel(filepath)\n",
    "    new_filename = filepath.stem\n",
    "    return csv_data.to_csv(f'../rawdata/{new_filename}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebe768ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing - it works!\n",
    "# convert_xlsx_to_csv(\"1999 Late Summer-Early Fall ESP Sampling.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d69a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013_EPA-DESA_Post_Hurricane_Sandy_Grab.xls\n",
      "2016_CPG_RM10_9_RA_SPME-Cap_Monitor.xls\n",
      "2012_LBG_Newark_Bay_SedFlume_Atterberg.xls\n"
     ]
    }
   ],
   "source": [
    "# Now I want to see if we can loop over this folder and convert all of them at once.\n",
    "xlsx_directory = '../rawdata/xlsx_to_convert'\n",
    "for filename in os.listdir(xlsx_directory):\n",
    "    if filename.endswith('.xls'):\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4517a35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm re-writing the function to include the for loop inside\n",
    "def convert_xlsx_to_csv(xlsx_directory):\n",
    "    for filename in os.listdir(xlsx_directory):\n",
    "        if filename.endswith('.xlsx'):\n",
    "            filepath = Path(f'../rawdata/xlsx_to_convert/{filename}')\n",
    "            csv_data = pd.read_excel(filepath)\n",
    "            new_filename = filepath.stem\n",
    "            csv_data.to_csv(f'../rawdata/{new_filename}.csv')\n",
    "        else:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd25d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_directory = '../rawdata/xlsx_to_convert'\n",
    "# convert_xlsx_to_csv(xlsx_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3168cab",
   "metadata": {},
   "source": [
    "### 2. Narrow down the columns in our csvs to just the columns we want\n",
    "Background:\n",
    "- In class on 4/2/22 we reviewed the original datasets in excel and identified all columns we want to include in our analysis.\n",
    "- The columns are:\n",
    "['LOC_NAME',\n",
    "'SAMPLE_DATE',\n",
    "'TASK_CODE', \n",
    "'ANALYTIC_METHOD', \n",
    "'CAS_RN', \n",
    "'CHEMICAL_NAME', \n",
    "'REPORT_RESULT_VALUE', \n",
    "'REPORT_RESULT_UNIT', \n",
    "'REPORT_RESULT_LIMIT', \n",
    "'DETECT_FLAG', \n",
    "'REPORTABLE_RESULT', \n",
    "'LONGITUDE', \n",
    "'LATITUDE']\n",
    "Now we want to write a function that slices the datasets into just those columns. Potential issues are:\n",
    "- Different column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1898be19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write function to open csvs\n",
    "def open_raw_csv(data_directory):\n",
    "    for filename in os.listdir(data_directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            filepath = Path(f'../rawdata/{filename}')\n",
    "            csv_data = pd.read_csv(filepath, parse_dates=True, infer_datetime_format=True)\n",
    "            our_columns = ['LOC_NAME', 'SAMPLE_DATE', 'TASK_CODE', 'ANALYTIC_METHOD', 'CAS_RN', 'CHEMICAL_NAME', 'REPORT_RESULT_VALUE', 'REPORT_RESULT_UNIT', 'REPORT_RESULT_LIMIT', 'DETECT_FLAG', 'REPORTABLE_RESULT', 'LONGITUDE', 'LATITUDE']\n",
    "            check = all(item in csv_data.columns for item in our_columns)\n",
    "            if check is True:\n",
    "                clean_data = csv_data[['LOC_NAME', 'SAMPLE_DATE', 'TASK_CODE', 'ANALYTIC_METHOD', 'CAS_RN', 'CHEMICAL_NAME', 'REPORT_RESULT_VALUE', 'REPORT_RESULT_UNIT', 'REPORT_RESULT_LIMIT', 'DETECT_FLAG', 'REPORTABLE_RESULT', 'LONGITUDE', 'LATITUDE']]\n",
    "                new_filename = filepath.stem\n",
    "                clean_data.to_csv(f'../cleandata/{new_filename}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cf2c087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/6r5nm8y56sbbg2kbt5s3pxhr0000gn/T/ipykernel_56994/2918624506.py:5: DtypeWarning: Columns (25,26,33,51,52,53,74,76) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  csv_data = pd.read_csv(filepath, parse_dates=True, infer_datetime_format=True)\n",
      "/var/folders/d9/6r5nm8y56sbbg2kbt5s3pxhr0000gn/T/ipykernel_56994/2918624506.py:5: DtypeWarning: Columns (68) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  csv_data = pd.read_csv(filepath, parse_dates=True, infer_datetime_format=True)\n",
      "/var/folders/d9/6r5nm8y56sbbg2kbt5s3pxhr0000gn/T/ipykernel_56994/2918624506.py:5: DtypeWarning: Columns (18,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  csv_data = pd.read_csv(filepath, parse_dates=True, infer_datetime_format=True)\n",
      "/var/folders/d9/6r5nm8y56sbbg2kbt5s3pxhr0000gn/T/ipykernel_56994/2918624506.py:5: DtypeWarning: Columns (14,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  csv_data = pd.read_csv(filepath, parse_dates=True, infer_datetime_format=True)\n",
      "/var/folders/d9/6r5nm8y56sbbg2kbt5s3pxhr0000gn/T/ipykernel_56994/2918624506.py:5: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  csv_data = pd.read_csv(filepath, parse_dates=True, infer_datetime_format=True)\n",
      "/var/folders/d9/6r5nm8y56sbbg2kbt5s3pxhr0000gn/T/ipykernel_56994/2918624506.py:5: DtypeWarning: Columns (22,36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  csv_data = pd.read_csv(filepath, parse_dates=True, infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "raw_data_dir = '../rawdata'\n",
    "for filename in os.listdir(raw_data_dir):\n",
    "        if filename.endswith('.csv'):\n",
    "            filepath = Path(f'../rawdata/{filename}')\n",
    "            csv_data = pd.read_csv(filepath, parse_dates=True, infer_datetime_format=True)\n",
    "            csv_data.columns = [col_name.upper() for col_name in csv_data.columns]\n",
    "            upper_columns = ['LOC_NAME', 'SAMPLE_DATE', 'TASK_CODE', 'ANALYTIC_METHOD', 'CAS_RN', 'CHEMICAL_NAME', 'REPORT_RESULT_VALUE', 'REPORT_RESULT_UNIT', 'REPORT_RESULT_LIMIT', 'DETECT_FLAG', 'REPORTABLE_RESULT', 'LONGITUDE', 'LATITUDE']\n",
    "            check = all(item in csv_data.columns for item in upper_columns)\n",
    "            if check is True:\n",
    "                clean_data = csv_data[upper_columns]\n",
    "                new_filename = filepath.stem\n",
    "                clean_data.to_csv(f'../cleandata/{new_filename}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fc34c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 Late Summer-Early Fall ESP Sampling.csv\n",
      "2015 ERT RM10.9 Suface Sediment Sampling.csv\n",
      "1990 Surficial Sediment Investigation.csv\n",
      "2010 CPG LPR-NB PWCM Field Measurements.csv\n",
      "2012 CPG Tissue Survey Above Dundee Dam.csv\n",
      "2018-2019_Diamond-Alkali_OU2_EPA-PDI-Split-samples.csv\n",
      "2012 CPG Benthic Sediment Above Dundee Dam.csv\n",
      "1995-96 Passaic Study RI_FS Sed Mobility EPA.csv\n",
      "2018-2019_OU2_PDI_Treatability Study_20210924.csv\n",
      "1981-2014_RPI(Bopp)SED-SUS_Matter.csv\n",
      "1999-2000 Minish Park Monitoring Program.csv\n",
      "2019_OU2_PDI_Porewater_Passive_Sampler_20210924.csv\n",
      "2001 Supplemental ESP Biota Sampling.csv\n",
      "1995 USACE Minish Park Investigation.csv\n",
      "2013_EPA-DESA_Post_Hurricane_Sandy_Grab.csv\n",
      "2010 CPG LPR-NB PWCM Sample Dataset.csv\n",
      "1997 Outfall Sampling Program.csv\n",
      "2000 Spring ESP Sampling Program.csv\n",
      "1991-1993 Core Sediment Investigations.csv\n",
      "1995 Sediment Grab Sampling Program.csv\n",
      "2012 OU3 NBSA SedFlume Atterberg Limits.csv\n",
      "2016_CPG_RM10_9_RA_SPME-Cap_Monitor.csv\n",
      "2012_LBG_Newark_Bay_SedFlume_Atterberg.csv\n",
      "1999 Sediment Sampling Program.csv\n",
      "2004 EarthTech SED Coring Pilot Project.csv\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(raw_data_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02f3cd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/6r5nm8y56sbbg2kbt5s3pxhr0000gn/T/ipykernel_56994/1946028009.py:2: DtypeWarning: Columns (14,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  csv_data = pd.read_csv(filepath, parse_dates=True, infer_datetime_format=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOC_NAME</th>\n",
       "      <th>SAMPLE_DATE</th>\n",
       "      <th>TASK_CODE</th>\n",
       "      <th>ANALYTIC_METHOD</th>\n",
       "      <th>CAS_RN</th>\n",
       "      <th>CHEMICAL_NAME</th>\n",
       "      <th>REPORT_RESULT_VALUE</th>\n",
       "      <th>REPORT_RESULT_UNIT</th>\n",
       "      <th>REPORT_RESULT_LIMIT</th>\n",
       "      <th>DETECT_FLAG</th>\n",
       "      <th>REPORTABLE_RESULT</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LATITUDE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RM 2.9</td>\n",
       "      <td>2018-11-18 11:40:00</td>\n",
       "      <td>2018-2019 OU2 PDI Treatability Study</td>\n",
       "      <td>E1668A</td>\n",
       "      <td>WHOTOTTEQ(H)1</td>\n",
       "      <td>Total WHO PCB/Dioxin TEQ(Human/Mammal)(ND=1)</td>\n",
       "      <td>304.0</td>\n",
       "      <td>pg/g</td>\n",
       "      <td>6.340</td>\n",
       "      <td>Y</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-74.12643</td>\n",
       "      <td>40.742445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RM 2.9</td>\n",
       "      <td>2018-11-18 11:40:00</td>\n",
       "      <td>2018-2019 OU2 PDI Treatability Study</td>\n",
       "      <td>E1668A</td>\n",
       "      <td>WHOTOTTEQ(H)0</td>\n",
       "      <td>Total WHO PCB/Dioxin TEQ(Human/Mammal)(ND=0)</td>\n",
       "      <td>304.0</td>\n",
       "      <td>pg/g</td>\n",
       "      <td>6.340</td>\n",
       "      <td>Y</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-74.12643</td>\n",
       "      <td>40.742445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RM 2.9</td>\n",
       "      <td>2018-11-18 11:40:00</td>\n",
       "      <td>2018-2019 OU2 PDI Treatability Study</td>\n",
       "      <td>E1668A</td>\n",
       "      <td>WHOTOTTEQ(H)5</td>\n",
       "      <td>Total WHO PCB/Dioxin TEQ(Human/Mammal)(ND=0.5)</td>\n",
       "      <td>304.0</td>\n",
       "      <td>pg/g</td>\n",
       "      <td>6.340</td>\n",
       "      <td>Y</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-74.12643</td>\n",
       "      <td>40.742445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RM 2.9</td>\n",
       "      <td>2018-11-18 11:40:00</td>\n",
       "      <td>2018-2019 OU2 PDI Treatability Study</td>\n",
       "      <td>E1668A</td>\n",
       "      <td>WHOTOTTEQ(F)5</td>\n",
       "      <td>Total WHO PCB/Dioxin TEQ(Fish)(ND=0.5)</td>\n",
       "      <td>279.0</td>\n",
       "      <td>pg/g</td>\n",
       "      <td>0.596</td>\n",
       "      <td>Y</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-74.12643</td>\n",
       "      <td>40.742445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RM 2.9</td>\n",
       "      <td>2018-11-18 11:40:00</td>\n",
       "      <td>2018-2019 OU2 PDI Treatability Study</td>\n",
       "      <td>E1668A</td>\n",
       "      <td>WHOTOTTEQ(F)1</td>\n",
       "      <td>Total WHO PCB/Dioxin TEQ(Fish)(ND=1)</td>\n",
       "      <td>280.0</td>\n",
       "      <td>pg/g</td>\n",
       "      <td>0.596</td>\n",
       "      <td>Y</td>\n",
       "      <td>Yes</td>\n",
       "      <td>-74.12643</td>\n",
       "      <td>40.742445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  LOC_NAME          SAMPLE_DATE                             TASK_CODE  \\\n",
       "0   RM 2.9  2018-11-18 11:40:00  2018-2019 OU2 PDI Treatability Study   \n",
       "1   RM 2.9  2018-11-18 11:40:00  2018-2019 OU2 PDI Treatability Study   \n",
       "2   RM 2.9  2018-11-18 11:40:00  2018-2019 OU2 PDI Treatability Study   \n",
       "3   RM 2.9  2018-11-18 11:40:00  2018-2019 OU2 PDI Treatability Study   \n",
       "4   RM 2.9  2018-11-18 11:40:00  2018-2019 OU2 PDI Treatability Study   \n",
       "\n",
       "  ANALYTIC_METHOD         CAS_RN  \\\n",
       "0          E1668A  WHOTOTTEQ(H)1   \n",
       "1          E1668A  WHOTOTTEQ(H)0   \n",
       "2          E1668A  WHOTOTTEQ(H)5   \n",
       "3          E1668A  WHOTOTTEQ(F)5   \n",
       "4          E1668A  WHOTOTTEQ(F)1   \n",
       "\n",
       "                                    CHEMICAL_NAME  REPORT_RESULT_VALUE  \\\n",
       "0    Total WHO PCB/Dioxin TEQ(Human/Mammal)(ND=1)                304.0   \n",
       "1    Total WHO PCB/Dioxin TEQ(Human/Mammal)(ND=0)                304.0   \n",
       "2  Total WHO PCB/Dioxin TEQ(Human/Mammal)(ND=0.5)                304.0   \n",
       "3          Total WHO PCB/Dioxin TEQ(Fish)(ND=0.5)                279.0   \n",
       "4            Total WHO PCB/Dioxin TEQ(Fish)(ND=1)                280.0   \n",
       "\n",
       "  REPORT_RESULT_UNIT  REPORT_RESULT_LIMIT DETECT_FLAG REPORTABLE_RESULT  \\\n",
       "0               pg/g                6.340           Y               Yes   \n",
       "1               pg/g                6.340           Y               Yes   \n",
       "2               pg/g                6.340           Y               Yes   \n",
       "3               pg/g                0.596           Y               Yes   \n",
       "4               pg/g                0.596           Y               Yes   \n",
       "\n",
       "   LONGITUDE   LATITUDE  \n",
       "0  -74.12643  40.742445  \n",
       "1  -74.12643  40.742445  \n",
       "2  -74.12643  40.742445  \n",
       "3  -74.12643  40.742445  \n",
       "4  -74.12643  40.742445  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = '/Users/laurenkrohn/Documents/GitHub-Local/project_1/environmental-contamination/data/rawdata/2018-2019_OU2_PDI_Treatability Study_20210924.csv'\n",
    "csv_data = pd.read_csv(filepath, parse_dates=True, infer_datetime_format=True)\n",
    "clean_data = csv_data[['LOC_NAME', 'SAMPLE_DATE', 'TASK_CODE', 'ANALYTIC_METHOD', 'CAS_RN', 'CHEMICAL_NAME', 'REPORT_RESULT_VALUE', 'REPORT_RESULT_UNIT', 'REPORT_RESULT_LIMIT', 'DETECT_FLAG', 'REPORTABLE_RESULT', 'LONGITUDE', 'LATITUDE']]\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08717ef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c8bdce3",
   "metadata": {},
   "source": [
    "def open_one_csv(filename):\n",
    "    for filename in os.listdir(data_directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            filepath = Path(f'../rawdata/{filename}')\n",
    "            csv_data = pd.read_csv(filepath, parse_dates=True, infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e11a41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/6r5nm8y56sbbg2kbt5s3pxhr0000gn/T/ipykernel_56994/3760036492.py:6: DtypeWarning: Columns (18,19,26,30,31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  csv_data = pd.read_csv(filepath, parse_dates=True, infer_datetime_format=True)\n",
      "/var/folders/d9/6r5nm8y56sbbg2kbt5s3pxhr0000gn/T/ipykernel_56994/3760036492.py:6: DtypeWarning: Columns (13,20,21,22,30,34,35,66,72,73) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  csv_data = pd.read_csv(filepath, parse_dates=True, infer_datetime_format=True)\n",
      "/var/folders/d9/6r5nm8y56sbbg2kbt5s3pxhr0000gn/T/ipykernel_56994/3760036492.py:6: DtypeWarning: Columns (18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  csv_data = pd.read_csv(filepath, parse_dates=True, infer_datetime_format=True)\n",
      "/var/folders/d9/6r5nm8y56sbbg2kbt5s3pxhr0000gn/T/ipykernel_56994/3760036492.py:6: DtypeWarning: Columns (18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  csv_data = pd.read_csv(filepath, parse_dates=True, infer_datetime_format=True)\n",
      "/var/folders/d9/6r5nm8y56sbbg2kbt5s3pxhr0000gn/T/ipykernel_56994/3760036492.py:6: DtypeWarning: Columns (72) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  csv_data = pd.read_csv(filepath, parse_dates=True, infer_datetime_format=True)\n",
      "/var/folders/d9/6r5nm8y56sbbg2kbt5s3pxhr0000gn/T/ipykernel_56994/3760036492.py:6: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  csv_data = pd.read_csv(filepath, parse_dates=True, infer_datetime_format=True)\n",
      "/var/folders/d9/6r5nm8y56sbbg2kbt5s3pxhr0000gn/T/ipykernel_56994/3760036492.py:6: DtypeWarning: Columns (22,73) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  csv_data = pd.read_csv(filepath, parse_dates=True, infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "# Use \"Column Selector\" for loop to pare down large csv datasets\n",
    "toobig_data_dir = '/Users/laurenkrohn/Documents/GitHub-Local/project_1/environmental-contamination/data/rawdata/too_big'\n",
    "for filename in os.listdir(toobig_data_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        filepath = Path(f'/Users/laurenkrohn/Documents/GitHub-Local/project_1/environmental-contamination/data/rawdata/too_big/{filename}')\n",
    "        csv_data = pd.read_csv(filepath, parse_dates=True, infer_datetime_format=True)\n",
    "        csv_data.columns = [col_name.upper() for col_name in csv_data.columns]\n",
    "        upper_columns = ['LOC_NAME', 'SAMPLE_DATE', 'TASK_CODE', 'ANALYTIC_METHOD', 'CAS_RN', 'CHEMICAL_NAME', 'REPORT_RESULT_VALUE', 'REPORT_RESULT_UNIT', 'REPORT_RESULT_LIMIT', 'DETECT_FLAG', 'REPORTABLE_RESULT', 'LONGITUDE', 'LATITUDE']\n",
    "        check = all(item in csv_data.columns for item in upper_columns)\n",
    "            if check is True:\n",
    "                clean_data = csv_data[upper_columns]\n",
    "                new_filename = filepath.stem\n",
    "                clean_data.to_csv(f'/Users/laurenkrohn/Documents/GitHub-Local/project_1/environmental-contamination/data/cleandata/{new_filename}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0b5c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 5 datasets that still need to be pared down because they are still over 50mb:\n",
    "# 2017-2019_OU2_PDI_Sediment_PCB_20210924.csv\n",
    "# 2018_GSH_LPR_8-3mile_data.csv\n",
    "# 2017_GSH_LPR_8-3mile_data.csv\n",
    "# 2008 CPG Low Res Coring Dataset.csv\n",
    "# 2017-2019_OU2_PDI_Sediment_NON_PCB_20210924.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "385789be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145270923"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = Path(f'/Users/laurenkrohn/Documents/GitHub-Local/project_1/environmental-contamination/data/cleandata/2017-2019_OU2_PDI_Sediment_PCB_20210924.csv')\n",
    "big_df = pd.read_csv(filepath, parse_dates=True, infer_datetime_format=True)\n",
    "\n",
    "big_df.iloc[:100000].to_csv('temp.csv')\n",
    "os.path.getsize('2017-2019_OU2_PDI_Sediment_PCB_20210924.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a11be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "17911712\n",
    "145270923\n",
    "50000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eba4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_max = int(5e5)\n",
    "filepath = Path(f'../rawdata/too_big/{filename}')\n",
    "row_from = 0\n",
    "row_to = rows_max\n",
    "file_n = 1\n",
    "\n",
    "while True:\n",
    "    fn_i = 'big_%s.csv' % str(file_n).zfill(3)\n",
    "    big_df.iloc[row_from:row_to].to_csv(fn_i)\n",
    "\n",
    "    if row_to > big_df.index.size:\n",
    "        break\n",
    "\n",
    "    row_from = row_to\n",
    "    row_to = row_from + rows_max\n",
    "    file_n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32584b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_toobig(data_directory):\n",
    "    for filename in os.listdir(data_directory):\n",
    "        filepath = Path(data_directory+'/'+ filename)\n",
    "        filesize = os.path.getsize(data_directory+'/'+ filename)\n",
    "        if filesize > int(5e7):\n",
    "            csv_data = pd.read_csv(filepath, parse_dates=True, infer_datetime_format=True)\n",
    "            csv_data.columns = [col_name.upper() for col_name in csv_data.columns]\n",
    "            upper_columns = ['LOC_NAME', 'SAMPLE_DATE', 'TASK_CODE', 'ANALYTIC_METHOD', 'CAS_RN', 'CHEMICAL_NAME', 'REPORT_RESULT_VALUE', 'REPORT_RESULT_UNIT', 'REPORT_RESULT_LIMIT', 'DETECT_FLAG', 'REPORTABLE_RESULT', 'LONGITUDE', 'LATITUDE']\n",
    "            check = all(item in csv_data.columns for item in upper_columns)\n",
    "            if check is True:\n",
    "                clean_data = csv_data[upper_columns]\n",
    "                new_filename = filepath.stem\n",
    "                clean_data.to_csv\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e06d5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'FACILITY_ID', 'FACILITY_CODE', 'SYS_LOC_CODE',\n",
       "       'LOC_NAME', 'SAMPLE_ID', 'SYS_SAMPLE_CODE', 'SAMPLE_NAME',\n",
       "       'SAMPLE_DATE', 'SAMPLEDATE', 'SAMPLETIME', 'SAMPLE_TYPE_CODE',\n",
       "       'START_DEPTH', 'END_DEPTH', 'DEPTH_UNIT', 'MATRIX_CODE', 'TASK_CODE',\n",
       "       'TASK_CODE_2', 'PARENT_SAMPLE_CODE', 'FIELD_SDG', 'ANALYSIS_LOCATION',\n",
       "       'LAB_SAMPLE_ID', 'LAB_MATRIX_CODE', 'LAB_NAME_CODE', 'ANALYTIC_METHOD',\n",
       "       'ANALYSIS_DATE', 'COLUMN_NUMBER', 'FRACTION', 'TEST_TYPE',\n",
       "       'PREP_METHOD', 'LEACHATE_METHOD', 'LEACHATE_DATE', 'LAB_SDG',\n",
       "       'PERCENT_MOISTURE', 'DILUTION_FACTOR', 'TEST_ID', 'CAS_RN',\n",
       "       'CHEMICAL_NAME', 'ORGANIC_YN', 'REPORT_RESULT_TEXT',\n",
       "       'REPORT_RESULT_VALUE', 'REPORT_RESULT_UNIT', 'REPORT_RESULT_LIMIT',\n",
       "       'REPORT_METHOD_DETECTION_LIMIT', 'REPORT_REPORTING_LIMIT',\n",
       "       'REPORT_QUANTITATION_LIMIT', 'REPORTABLE_RESULT', 'DETECT_FLAG',\n",
       "       'INTERPRETED_QUALIFIERS', 'VALIDATOR_QUALIFIERS', 'LAB_QUALIFIERS',\n",
       "       'QUANTITATION_LIMIT', 'METHOD_DETECTION_LIMIT',\n",
       "       'REPORTING_DETECTION_LIMIT', 'DETECTION_LIMIT_UNIT', 'APPROVAL_CODE',\n",
       "       'RESULT_TEXT', 'RESULT_NUMERIC', 'RESULT_UNIT', 'RESULT_TYPE_CODE',\n",
       "       'LONGITUDE', 'LATITUDE', 'VALIDATED_YN', 'STREAM_CODE', 'STREAM_MILE',\n",
       "       'REMARK'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_directory = '/Users/laurenkrohn/Documents/GitHub-Local/project_1/environmental-contamination/data/rawdata/too_big'\n",
    "filename = '2017-2019_OU2_PDI_Sediment_PCB_20210924.csv'\n",
    "filesize = os.path.getsize(data_directory+'/'+ filename)\n",
    "\n",
    "filepath = Path(data_directory+'/'+ filename)\n",
    "if filesize > int(5e7):\n",
    "    csv_data = pd.read_csv(filepath, parse_dates=True, infer_datetime_format=True, low_memory=False)\n",
    "    csv_data.head()\n",
    "\n",
    "original_columns = csv_data.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5110d6b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 66 elements, new values have 13 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, number_lines, rows_max):\n\u001b[1;32m      5\u001b[0m     csv_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(filepath, \n\u001b[1;32m      6\u001b[0m         nrows\u001b[39m=\u001b[39mrows_max,\n\u001b[1;32m      7\u001b[0m         skiprows\u001b[39m=\u001b[39mi,\n\u001b[1;32m      8\u001b[0m         parse_dates\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, infer_datetime_format\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, low_memory\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m----> 9\u001b[0m     csv_data\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m header\n\u001b[1;32m     10\u001b[0m     new_filename \u001b[39m=\u001b[39m filepath\u001b[39m.\u001b[39mstem\n\u001b[1;32m     11\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(i)\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m: \n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:5596\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/laurenkrohn/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py?line=5593'>5594</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/laurenkrohn/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py?line=5594'>5595</a>\u001b[0m     \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(\u001b[39mself\u001b[39m, name)\n\u001b[0;32m-> <a href='file:///Users/laurenkrohn/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py?line=5595'>5596</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__setattr__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name, value)\n\u001b[1;32m   <a href='file:///Users/laurenkrohn/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py?line=5596'>5597</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/laurenkrohn/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py?line=5597'>5598</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/properties.pyx:70\u001b[0m, in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:769\u001b[0m, in \u001b[0;36mNDFrame._set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/laurenkrohn/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py?line=766'>767</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_axis\u001b[39m(\u001b[39mself\u001b[39m, axis: \u001b[39mint\u001b[39m, labels: Index) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/laurenkrohn/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py?line=767'>768</a>\u001b[0m     labels \u001b[39m=\u001b[39m ensure_index(labels)\n\u001b[0;32m--> <a href='file:///Users/laurenkrohn/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py?line=768'>769</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mset_axis(axis, labels)\n\u001b[1;32m    <a href='file:///Users/laurenkrohn/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py?line=769'>770</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py:214\u001b[0m, in \u001b[0;36mBaseBlockManager.set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/laurenkrohn/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py?line=211'>212</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_axis\u001b[39m(\u001b[39mself\u001b[39m, axis: \u001b[39mint\u001b[39m, new_labels: Index) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/laurenkrohn/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py?line=212'>213</a>\u001b[0m     \u001b[39m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/laurenkrohn/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py?line=213'>214</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_set_axis(axis, new_labels)\n\u001b[1;32m    <a href='file:///Users/laurenkrohn/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py?line=214'>215</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis] \u001b[39m=\u001b[39m new_labels\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/base.py:69\u001b[0m, in \u001b[0;36mDataManager._validate_set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/laurenkrohn/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/base.py?line=65'>66</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/laurenkrohn/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/base.py?line=67'>68</a>\u001b[0m \u001b[39melif\u001b[39;00m new_len \u001b[39m!=\u001b[39m old_len:\n\u001b[0;32m---> <a href='file:///Users/laurenkrohn/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/base.py?line=68'>69</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     <a href='file:///Users/laurenkrohn/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/base.py?line=69'>70</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLength mismatch: Expected axis has \u001b[39m\u001b[39m{\u001b[39;00mold_len\u001b[39m}\u001b[39;00m\u001b[39m elements, new \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/laurenkrohn/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/base.py?line=70'>71</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalues have \u001b[39m\u001b[39m{\u001b[39;00mnew_len\u001b[39m}\u001b[39;00m\u001b[39m elements\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/laurenkrohn/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/base.py?line=71'>72</a>\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 66 elements, new values have 13 elements"
     ]
    }
   ],
   "source": [
    "\n",
    "rows_max = int(1e5)\n",
    "number_lines = sum(1 for row in (open(filename)))\n",
    "header = ['LOC_NAME', 'SAMPLE_DATE', 'TASK_CODE', 'ANALYTIC_METHOD', 'CAS_RN', 'CHEMICAL_NAME', 'REPORT_RESULT_VALUE', 'REPORT_RESULT_UNIT', 'REPORT_RESULT_LIMIT', 'DETECT_FLAG', 'REPORTABLE_RESULT', 'LONGITUDE', 'LATITUDE']\n",
    "for i in range(1, number_lines, rows_max):\n",
    "    csv_data = pd.read_csv(filepath, \n",
    "        nrows=rows_max,\n",
    "        skiprows=i,\n",
    "        parse_dates=True, infer_datetime_format=True, low_memory=False)\n",
    "    csv_data.columns = [col_name.upper() for col_name in csv_data.columns]\n",
    "        upper_columns = ['LOC_NAME', 'SAMPLE_DATE', 'TASK_CODE', 'ANALYTIC_METHOD', 'CAS_RN', 'CHEMICAL_NAME', 'REPORT_RESULT_VALUE', 'REPORT_RESULT_UNIT', 'REPORT_RESULT_LIMIT', 'DETECT_FLAG', 'REPORTABLE_RESULT', 'LONGITUDE', 'LATITUDE']\n",
    "        check = all(item in csv_data.columns for item in upper_columns)\n",
    "            if check is True:\n",
    "                clean_data = csv_data[upper_columns]\n",
    "    csv_data.columns = header\n",
    "    new_filename = filepath.stem\n",
    "    if str(i)=='1': \n",
    "        csv_number = '1'\n",
    "    else: \n",
    "        csv_number = str(int(i/100000))\n",
    "    out_csv = new_filename + '_' + csv_number + '.csv'\n",
    "    csv_data.to_csv(out_csv, index=False, header=True, mode='a', chunksize=rows_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "31eaa315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163008046\n",
      "6148\n",
      "318898875\n",
      "62861525\n",
      "396388001\n",
      "122218167\n",
      "194735844\n",
      "53968521\n",
      "266428971\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(toobig_data_dir):\n",
    "    filesize = os.path.getsize(toobig_data_dir+'/'+ filename)\n",
    "    print(filesize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a3c769ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Operative function to clean too big csvs\n",
    "toobig_data_dir = '../rawdata/too_big'\n",
    "for filename in os.listdir(toobig_data_dir):\n",
    "    filepath = Path(toobig_data_dir+'/'+ filename)\n",
    "    if filename.endswith('.csv'):\n",
    "        big_data = pd.read_csv(filepath, parse_dates=True, infer_datetime_format=True, low_memory=False)\n",
    "    big_data.columns = [col_name.upper() for col_name in big_data.columns]\n",
    "    upper_columns = ['LOC_NAME', 'SAMPLE_DATE', 'TASK_CODE', 'ANALYTIC_METHOD', 'CAS_RN', 'CHEMICAL_NAME', 'REPORT_RESULT_VALUE', 'REPORT_RESULT_UNIT', 'REPORT_RESULT_LIMIT', 'DETECT_FLAG', 'REPORTABLE_RESULT', 'LONGITUDE', 'LATITUDE']\n",
    "    check = all(item in csv_data.columns for item in upper_columns)\n",
    "    if check is True:\n",
    "        big_clean_data = big_data[upper_columns]\n",
    "    new_filename = filepath.stem\n",
    "    for i in range(0, len(big_clean_data), int(2e5)):\n",
    "        csv_number = str(int(i/200000))\n",
    "        big_clean_data.iloc[i:i+int(2e5)].to_csv(f'{new_filename}_{csv_number}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d74384cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "i=200000\n",
    "if str(i)=='1': \n",
    "    print('1') \n",
    "else: \n",
    "    print(str(int(i/100000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf7e235e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (207796089.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [50]\u001b[0;36m\u001b[0m\n\u001b[0;31m    csv_number =\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "csv_number = \n",
    "    if str(i)=='1': \n",
    "        '1'\n",
    "    else: \n",
    "        str(int(i/100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb45e105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
